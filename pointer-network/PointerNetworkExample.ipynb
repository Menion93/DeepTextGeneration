{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embeddings(embed_matrix, trainable=False):\n",
    "    return keras.layers.Embedding(embed_matrix.shape[0],\n",
    "                                  embed_matrix.shape[1],\n",
    "                                  weights=[embed_matrix],\n",
    "                                  trainable=trainable)\n",
    "\n",
    "def get_new_embeddings(voc_len, embedding_dim):\n",
    "    return keras.layers.Embedding(voc_len, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.lstm = keras.layers.LSTM(self.units,\n",
    "                                      return_sequences=True, \n",
    "                                      return_state=True)\n",
    "        \n",
    "    def call(self, x):\n",
    "        sequences, state1, state2 = self.lstm(x)\n",
    "        return sequences, state1, state2\n",
    "    \n",
    "class Attention(keras.Model):\n",
    "    def __init__(self, w_units):\n",
    "        super().__init__()\n",
    "        self.W = keras.layers.Dense(w_units, use_bias=False)\n",
    "        self.W1 = keras.layers.Dense(w_units, use_bias=False)\n",
    "        self.v = keras.layers.Dense(1)\n",
    "        self.tanh = keras.activations.tanh\n",
    "        self.softmax = keras.activations.softmax\n",
    "        \n",
    "    def call(self, enc_hidden, dec_hidden):\n",
    "        dec_hidden = tf.expand_dims(dec_hidden, 1)\n",
    "        unnorm = self.v(\n",
    "            self.tanh(self.W(enc_hidden) + self.W1(dec_hidden))\n",
    "        )\n",
    "\n",
    "        attention_weights = self.softmax(unnorm, axis=1)\n",
    "        \n",
    "        # Compute the context vector used to generate the decoder state\n",
    "        c_vec = tf.reduce_sum(attention_weights * enc_hidden, axis=1)\n",
    "        \n",
    "        # Return the context vector and the pointer logits and the pointer probs\n",
    "        return c_vec, tf.squeeze(attention_weights, axis=2)  \n",
    "    \n",
    "class Decoder(keras.Model):\n",
    "    def __init__(self, units, output_size):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "        self.lstm = keras.layers.LSTM(self.units, return_state=True)\n",
    "        self.output_layer = keras.layers.Dense(output_size, activation='softmax')\n",
    "        \n",
    "    def call(self, x, enc_out, prev_state):\n",
    "        # Concatenate encoder output (or context vector) and the target/predicted embedding\n",
    "        concatenated_inp = tf.concat([x, enc_out], axis=1)\n",
    "        concatenated_inp = tf.expand_dims(concatenated_inp, 1)\n",
    "        # Compute the hidden state h^d\n",
    "        d, dec_h, dec_c =  self.lstm(concatenated_inp, initial_state=prev_state)\n",
    "        \n",
    "        # Decode using vocabulary\n",
    "        flattened = tf.layers.flatten(d)\n",
    "        decoded_probs = self.output_layer(flattened)\n",
    "        \n",
    "        # Return Decode hidden states and vocabulary logits over the vocabulary\n",
    "        return d, dec_h, dec_c, decoded_probs\n",
    "\n",
    "class PointerSwitch(keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = keras.layers.Dense(units,  use_bias=False)\n",
    "        self.W2 = keras.layers.Dense(units, use_bias=False)\n",
    "        self.v = keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, enc, c_vec):\n",
    "        '''\n",
    "            Compute switch probabilities from the context vector and\n",
    "            the encoder last output state\n",
    "        '''\n",
    "        switch_prob = tf.keras.activations.sigmoid(\n",
    "            self.v(\n",
    "                self.W1(enc) + self.W2(c_vec)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return switch_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointerNetwork(keras.Model):\n",
    "    def __init__(self,\n",
    "                 enc_units,\n",
    "                 dec_units, \n",
    "                 voc_size,\n",
    "                 att_units, \n",
    "                 switch_units,\n",
    "                 max_len, \n",
    "                 start_token,\n",
    "                 end_token,\n",
    "                 padding_char):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(enc_units)\n",
    "        self.decoder = Decoder(dec_units, voc_size)\n",
    "        self.attention = Attention(att_units)\n",
    "        self.pointer_switch = PointerSwitch(switch_units)\n",
    "        self.embeddings = False\n",
    "        self.max_len = max_len\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.voc_size = voc_size\n",
    "        self.padding_char = padding_char\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer()\n",
    "    \n",
    "    def set_embeddings_layer(self, embeddings_layer):\n",
    "        self.embeddings = embeddings_layer\n",
    "    \n",
    "    def predict_batch(self, X):\n",
    "        assert self.embeddings, \"Call self.set_embeddings_layer first\"\n",
    "        X = tf.convert_to_tensor(X)\n",
    "        \n",
    "        embed = self.embeddings(X)\n",
    "        enc_states, h1, h2 = self.encoder(embed)\n",
    "        input_tokens = tf.convert_to_tensor([self.start_token] * embed.shape[0])\n",
    "        # put last encoder state as attention vec at start\n",
    "        c_vec = h1\n",
    "        outputs = []\n",
    "        \n",
    "        for _ in range(self.max_len):\n",
    "            dec_input = self.embeddings(input_tokens)\n",
    "            decoded_state, h1, h2, decoded_probs = self.decoder(dec_input, \n",
    "                                                                c_vec, \n",
    "                                                                [h1, h2])\n",
    "            c_vec, pointer_probs = self.attention(enc_states, \n",
    "                                                     decoded_state)\n",
    "            \n",
    "            # Compute switch probability to decide where to extract the next\n",
    "            # word token\n",
    "            switch_probs = self.pointer_switch(h1, c_vec)\n",
    "            # Decode based on switch probs            \n",
    "            input_tokens = self.decode_next_word(switch_probs, \n",
    "                                                 decoded_probs,\n",
    "                                                 X,\n",
    "                                                 pointer_probs)\n",
    "            outputs.append(input_tokens)\n",
    "            \n",
    "        return tf.transpose(tf.convert_to_tensor(outputs))\n",
    "    \n",
    "    def decode_next_word(self, switch_probs, decoded_probs, inputs, att_probs):\n",
    "        sampled_probs = tf.random.uniform(switch_probs.shape, 0 , 1)\n",
    "        tokens = []\n",
    "        token = None\n",
    "\n",
    "        for prob, sampled, decoded, inp, att_p in zip(switch_probs,\n",
    "                                                      sampled_probs, \n",
    "                                                      decoded_probs,\n",
    "                                                      inputs,\n",
    "                                                      att_probs):\n",
    "            if prob.numpy()[0] >= sampled.numpy()[0]:\n",
    "                token = self.fixed_vocab_decode(decoded)\n",
    "            else:\n",
    "                token = self.pointer_greedy_search(att_p, inp)\n",
    "\n",
    "            tokens.append(token)\n",
    "            \n",
    "        return tf.convert_to_tensor(tokens, dtype=tf.float32)\n",
    "    \n",
    "    def pointer_greedy_search(self, probs, inputs):\n",
    "        return inputs[tf.argmax(probs)]\n",
    "    \n",
    "    def fixed_vocab_decode(self, decoded_probs):\n",
    "        return tf.argmax(decoded_probs)\n",
    "    \n",
    "    def pointer_batch_loss(self, gen, y, d_prob, p_prob, s_prob):\n",
    "        # Compute the mask to ignore the padding in the loss\n",
    "        mask = 1-tf.cast(tf.equal(gen[:,None],\n",
    "                                  tf.ones(gen[:,None].shape) * self.padding_char\n",
    "                                 ), tf.float32)\n",
    "        \n",
    "        # Compute pointer loss across all values of y for the pointer and generated probs\n",
    "        pointer_mat = (p_prob + (1 - s_prob)) * mask\n",
    "        generator_mat = (d_prob + s_prob) * mask\n",
    "        \n",
    "        # Add the expected loss in terms of likelihood\n",
    "        batch_loss = 0\n",
    "        for i, g in enumerate(gen):\n",
    "            # Add if the word was taken from the input\n",
    "            if g == 0:\n",
    "                batch_loss += pointer_mat[i, y[i]]\n",
    "            # Add if the word was generated by the network\n",
    "            else:\n",
    "                batch_loss += generator_mat[i, y[i]]\n",
    "            \n",
    "        # Reduce to scalar, dont forget to include minus sign (its a loss not a likelihood)\n",
    "        return -batch_loss\n",
    "    \n",
    "    def __train_batch(self, X, y, gen):\n",
    "        assert self.embeddings, \"Call self.load_embeddings first\"\n",
    "\n",
    "        X = tf.convert_to_tensor(X)\n",
    "        y = tf.convert_to_tensor(y, dtype='int32')\n",
    "        gen = tf.convert_to_tensor(gen, dtype='float32')\n",
    "\n",
    "        enc_inp = self.embeddings(X)\n",
    "        enc_states, h1, h2 = self.encoder(enc_inp)\n",
    "        c_vec = h1\n",
    "        input_tokens = y[:,0]\n",
    "        loss = 0\n",
    "        for t in range(1, y.shape[1]):\n",
    "            # Get embeddings\n",
    "            dec_input = self.embeddings(input_tokens)\n",
    "            \n",
    "            # Get decoder output\n",
    "            decoded_state, h1, h2, decoded_probs = self.decoder(dec_input, c_vec, [h1, h2])\n",
    "            \n",
    "            # Get context vector for the next step, and pointer probabilities\n",
    "            c_vec, pointer_probs = self.attention(enc_states, decoded_state)\n",
    "            \n",
    "            # Get switch probability (BS*1)\n",
    "            switch_probs = self.pointer_switch(h1, c_vec)\n",
    "                        \n",
    "            # Is target generated or extracted from the input (BS * 1)\n",
    "            batch_gen = tf.convert_to_tensor(gen[:, t])\n",
    "            \n",
    "            # Compute Pointer Network batch loss at timestep t\n",
    "            loss += self.pointer_batch_loss(batch_gen, y[:, t], decoded_probs,\n",
    "                                       pointer_probs, switch_probs)\n",
    "\n",
    "            # Get next decoder input tokens\n",
    "            input_tokens = y[:, t]\n",
    "        \n",
    "        # Dont forget to divide by summary lenght N, since we lose the /N component n by calling\n",
    "        # N times softmax cross entropy\n",
    "        loss = loss / int(y.shape[1]-1)\n",
    "        print(loss)\n",
    "        return loss\n",
    "    \n",
    "    def train_batch(self, X, y, gen):\n",
    "        return self.optimizer.minimize(lambda: self.__train_batch(X, y, gen))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_units = 128\n",
    "dec_units = 128\n",
    "voc_size = 300\n",
    "att_units = 128 \n",
    "switch_units = 128\n",
    "max_len = 200\n",
    "start_index_token = 0\n",
    "end_index_token = 1\n",
    "padding_char = -1\n",
    "ptr = PointerNetwork(enc_units, \n",
    "                     dec_units, \n",
    "                     voc_size, \n",
    "                     att_units, \n",
    "                     switch_units, \n",
    "                     max_len, \n",
    "                     start_index_token, \n",
    "                     end_index_token,\n",
    "                     padding_char)\n",
    "\n",
    "ptr.set_embeddings_layer(load_pretrained_embeddings(np.ones((300,300))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=44505859, shape=(2, 200), dtype=float32, numpy=\n",
       "array([[  1.,   1., 285., 285.,   1., 112., 237.,  28., 104.,  55.,  55.,\n",
       "          1.,  55.,   1.,   1.,   1., 129., 177., 177.,   1.,   1., 129.,\n",
       "        177.,   1.,   1.,   1., 129.,   1., 177., 177., 177.,   1.,   1.,\n",
       "          1.,   1., 129., 177., 177.,   1., 177., 268.,   1.,  90.,   1.,\n",
       "         98.,   1., 129.,   1.,   1., 129.,   1.,   1.,   1.,   1.,   1.,\n",
       "          1., 129.,   1.,   1., 134.,   1., 134.,   1.,   1., 134.,   1.,\n",
       "          1., 134., 134., 134., 215., 105.,   1.,   1.,   1.,   1.,   1.,\n",
       "        134., 134., 134., 215., 105.,  55.,  55.,   1.,   1.,   1., 129.,\n",
       "          1.,   1.,   1., 134.,   1.,   1., 134., 134.,   1., 134., 134.,\n",
       "          1.,   1., 134.,   1.,   1.,   1.,   1., 134., 134.,   1.,   1.,\n",
       "        134.,   1.,   1., 134., 134.,   1.,   1., 134., 134.,   1.,   1.,\n",
       "        134., 134.,   1.,   1.,   1., 134., 134., 134., 215.,   1., 134.,\n",
       "        134.,   1.,   1.,   1.,   1., 134., 134., 134., 215., 105.,   1.,\n",
       "          1.,   1., 134.,   1., 134.,   1.,   1.,   1.,   1.,   1., 134.,\n",
       "        134., 134.,   1.,   1.,   1., 134.,   1.,   1.,   1.,   1.,   1.,\n",
       "          1.,   1., 134.,   1.,   1., 134., 134., 215.,   1., 134.,   1.,\n",
       "        134.,   1., 134., 134., 134.,   1., 134.,   1., 134., 134.,   1.,\n",
       "          1., 134.,   1., 134., 134.,   1.,   1.,   1., 134., 134.,   1.,\n",
       "          1.,   1.],\n",
       "       [  1., 285.,   1., 285., 285.,   1.,   1.,   1., 129., 177.,   1.,\n",
       "        177.,   1., 254.,   1.,   1.,   1., 129.,   1., 134., 134., 134.,\n",
       "        215., 184., 134., 163.,   1.,   1.,   1.,   1.,   1., 134., 134.,\n",
       "        134., 215., 134.,   1., 134.,   1., 134., 134., 134., 215.,   1.,\n",
       "        134., 134.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1., 134.,\n",
       "          1.,   1.,   1.,   1., 134.,   1., 134.,   1., 134.,   1.,   1.,\n",
       "          1.,   1., 134.,   1., 134.,   1., 134., 134.,   1., 134., 134.,\n",
       "        215.,   1., 134., 134., 134.,   1.,   1.,   1., 134.,   1.,   1.,\n",
       "          1.,   1., 134.,   1., 134.,   1., 134., 134.,   1., 134.,   1.,\n",
       "          1., 134.,   1.,   1., 134., 134.,   1.,   1.,   1.,   1.,   1.,\n",
       "        134., 134.,   1., 134.,   1.,   1.,   1.,   1.,   1., 134.,   1.,\n",
       "        134.,   1., 134.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,   1.,\n",
       "          1.,   1., 134.,   1.,   1.,   1., 134.,   1.,   1., 134., 134.,\n",
       "        215.,   1.,   1., 134., 134., 134.,   1., 134.,   1.,   1.,   1.,\n",
       "        134., 134.,   1.,   1., 134., 134.,   1., 134.,   1., 134.,   1.,\n",
       "        134.,   1., 134.,   1., 134., 134.,   1.,   1., 134., 134., 134.,\n",
       "          1., 134.,   1., 134., 134., 215., 105.,   1.,  55.,   1.,   1.,\n",
       "        134.,   1., 134., 134., 134.,   1., 134., 134., 134., 215.,   1.,\n",
       "          1., 134.]], dtype=float32)>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptr.predict_batch(tf.convert_to_tensor(np.ones((2,10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = [np.random.randint(0,300, size=20)] * 32\n",
    "y = [[0, 1, 22, 44, 87,1]] * 32\n",
    "gen = np.array([np.random.randint(0,2, size=6) for _ in range(32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-16.049234, shape=(), dtype=float32)\n",
      "tf.Tensor(-16.840847, shape=(), dtype=float32)\n",
      "tf.Tensor(-17.686268, shape=(), dtype=float32)\n",
      "tf.Tensor(-18.710785, shape=(), dtype=float32)\n",
      "tf.Tensor(-20.022306, shape=(), dtype=float32)\n",
      "tf.Tensor(-21.718853, shape=(), dtype=float32)\n",
      "tf.Tensor(-23.85989, shape=(), dtype=float32)\n",
      "tf.Tensor(-26.376368, shape=(), dtype=float32)\n",
      "tf.Tensor(-28.9325, shape=(), dtype=float32)\n",
      "tf.Tensor(-30.909428, shape=(), dtype=float32)\n",
      "tf.Tensor(-31.876163, shape=(), dtype=float32)\n",
      "tf.Tensor(-32.1362, shape=(), dtype=float32)\n",
      "tf.Tensor(-32.21064, shape=(), dtype=float32)\n",
      "tf.Tensor(-32.30232, shape=(), dtype=float32)\n",
      "tf.Tensor(-32.449722, shape=(), dtype=float32)\n",
      "tf.Tensor(-32.65644, shape=(), dtype=float32)\n",
      "tf.Tensor(-32.92397, shape=(), dtype=float32)\n",
      "tf.Tensor(-33.261696, shape=(), dtype=float32)\n",
      "tf.Tensor(-33.692955, shape=(), dtype=float32)\n",
      "tf.Tensor(-34.2498, shape=(), dtype=float32)\n",
      "tf.Tensor(-34.944523, shape=(), dtype=float32)\n",
      "tf.Tensor(-35.792034, shape=(), dtype=float32)\n",
      "tf.Tensor(-36.737633, shape=(), dtype=float32)\n",
      "tf.Tensor(-37.693806, shape=(), dtype=float32)\n",
      "tf.Tensor(-38.664005, shape=(), dtype=float32)\n",
      "tf.Tensor(-39.592365, shape=(), dtype=float32)\n",
      "tf.Tensor(-40.45881, shape=(), dtype=float32)\n",
      "tf.Tensor(-41.230305, shape=(), dtype=float32)\n",
      "tf.Tensor(-41.91198, shape=(), dtype=float32)\n",
      "tf.Tensor(-42.517975, shape=(), dtype=float32)\n",
      "tf.Tensor(-43.012608, shape=(), dtype=float32)\n",
      "tf.Tensor(-43.430344, shape=(), dtype=float32)\n",
      "tf.Tensor(-43.795868, shape=(), dtype=float32)\n",
      "tf.Tensor(-44.117973, shape=(), dtype=float32)\n",
      "tf.Tensor(-44.39026, shape=(), dtype=float32)\n",
      "tf.Tensor(-44.614555, shape=(), dtype=float32)\n",
      "tf.Tensor(-44.80426, shape=(), dtype=float32)\n",
      "tf.Tensor(-44.973175, shape=(), dtype=float32)\n",
      "tf.Tensor(-45.143276, shape=(), dtype=float32)\n",
      "tf.Tensor(-45.311882, shape=(), dtype=float32)\n",
      "tf.Tensor(-45.48107, shape=(), dtype=float32)\n",
      "tf.Tensor(-45.653828, shape=(), dtype=float32)\n",
      "tf.Tensor(-45.82537, shape=(), dtype=float32)\n",
      "tf.Tensor(-45.986286, shape=(), dtype=float32)\n",
      "tf.Tensor(-46.15677, shape=(), dtype=float32)\n",
      "tf.Tensor(-46.32409, shape=(), dtype=float32)\n",
      "tf.Tensor(-46.486187, shape=(), dtype=float32)\n",
      "tf.Tensor(-46.648476, shape=(), dtype=float32)\n",
      "tf.Tensor(-46.807495, shape=(), dtype=float32)\n",
      "tf.Tensor(-46.9699, shape=(), dtype=float32)\n",
      "tf.Tensor(-47.130707, shape=(), dtype=float32)\n",
      "tf.Tensor(-47.29606, shape=(), dtype=float32)\n",
      "tf.Tensor(-47.46872, shape=(), dtype=float32)\n",
      "tf.Tensor(-47.64895, shape=(), dtype=float32)\n",
      "tf.Tensor(-47.819557, shape=(), dtype=float32)\n",
      "tf.Tensor(-47.986717, shape=(), dtype=float32)\n",
      "tf.Tensor(-48.150757, shape=(), dtype=float32)\n",
      "tf.Tensor(-48.30407, shape=(), dtype=float32)\n",
      "tf.Tensor(-48.453144, shape=(), dtype=float32)\n",
      "tf.Tensor(-48.59742, shape=(), dtype=float32)\n",
      "tf.Tensor(-48.74404, shape=(), dtype=float32)\n",
      "tf.Tensor(-48.895073, shape=(), dtype=float32)\n",
      "tf.Tensor(-49.0429, shape=(), dtype=float32)\n",
      "tf.Tensor(-49.191322, shape=(), dtype=float32)\n",
      "tf.Tensor(-49.338367, shape=(), dtype=float32)\n",
      "tf.Tensor(-49.472153, shape=(), dtype=float32)\n",
      "tf.Tensor(-49.602684, shape=(), dtype=float32)\n",
      "tf.Tensor(-49.73536, shape=(), dtype=float32)\n",
      "tf.Tensor(-49.86689, shape=(), dtype=float32)\n",
      "tf.Tensor(-49.99321, shape=(), dtype=float32)\n",
      "tf.Tensor(-50.116688, shape=(), dtype=float32)\n",
      "tf.Tensor(-50.24069, shape=(), dtype=float32)\n",
      "tf.Tensor(-50.36416, shape=(), dtype=float32)\n",
      "tf.Tensor(-50.478043, shape=(), dtype=float32)\n",
      "tf.Tensor(-50.593666, shape=(), dtype=float32)\n",
      "tf.Tensor(-50.704414, shape=(), dtype=float32)\n",
      "tf.Tensor(-50.814774, shape=(), dtype=float32)\n",
      "tf.Tensor(-50.926647, shape=(), dtype=float32)\n",
      "tf.Tensor(-51.039482, shape=(), dtype=float32)\n",
      "tf.Tensor(-51.14714, shape=(), dtype=float32)\n",
      "tf.Tensor(-51.251415, shape=(), dtype=float32)\n",
      "tf.Tensor(-51.357525, shape=(), dtype=float32)\n",
      "tf.Tensor(-51.464607, shape=(), dtype=float32)\n",
      "tf.Tensor(-51.571117, shape=(), dtype=float32)\n",
      "tf.Tensor(-51.679096, shape=(), dtype=float32)\n",
      "tf.Tensor(-51.783813, shape=(), dtype=float32)\n",
      "tf.Tensor(-51.88793, shape=(), dtype=float32)\n",
      "tf.Tensor(-51.99422, shape=(), dtype=float32)\n",
      "tf.Tensor(-52.09858, shape=(), dtype=float32)\n",
      "tf.Tensor(-52.198956, shape=(), dtype=float32)\n",
      "tf.Tensor(-52.305702, shape=(), dtype=float32)\n",
      "tf.Tensor(-52.41152, shape=(), dtype=float32)\n",
      "tf.Tensor(-52.520653, shape=(), dtype=float32)\n",
      "tf.Tensor(-52.628883, shape=(), dtype=float32)\n",
      "tf.Tensor(-52.743786, shape=(), dtype=float32)\n",
      "tf.Tensor(-52.855743, shape=(), dtype=float32)\n",
      "tf.Tensor(-52.974987, shape=(), dtype=float32)\n",
      "tf.Tensor(-53.08667, shape=(), dtype=float32)\n",
      "tf.Tensor(-53.204285, shape=(), dtype=float32)\n",
      "tf.Tensor(-53.325417, shape=(), dtype=float32)\n",
      "tf.Tensor(-53.44995, shape=(), dtype=float32)\n",
      "tf.Tensor(-53.581085, shape=(), dtype=float32)\n",
      "tf.Tensor(-53.707325, shape=(), dtype=float32)\n",
      "tf.Tensor(-53.84124, shape=(), dtype=float32)\n",
      "tf.Tensor(-53.97422, shape=(), dtype=float32)\n",
      "tf.Tensor(-54.093636, shape=(), dtype=float32)\n",
      "tf.Tensor(-54.24401, shape=(), dtype=float32)\n",
      "tf.Tensor(-54.392815, shape=(), dtype=float32)\n",
      "tf.Tensor(-54.53136, shape=(), dtype=float32)\n",
      "tf.Tensor(-54.688263, shape=(), dtype=float32)\n",
      "tf.Tensor(-54.836792, shape=(), dtype=float32)\n",
      "tf.Tensor(-54.98952, shape=(), dtype=float32)\n",
      "tf.Tensor(-55.135384, shape=(), dtype=float32)\n",
      "tf.Tensor(-55.275665, shape=(), dtype=float32)\n",
      "tf.Tensor(-55.408825, shape=(), dtype=float32)\n",
      "tf.Tensor(-55.544243, shape=(), dtype=float32)\n",
      "tf.Tensor(-55.673897, shape=(), dtype=float32)\n",
      "tf.Tensor(-55.80494, shape=(), dtype=float32)\n",
      "tf.Tensor(-55.92649, shape=(), dtype=float32)\n",
      "tf.Tensor(-56.051647, shape=(), dtype=float32)\n",
      "tf.Tensor(-56.17121, shape=(), dtype=float32)\n",
      "tf.Tensor(-56.278465, shape=(), dtype=float32)\n",
      "tf.Tensor(-56.36263, shape=(), dtype=float32)\n",
      "tf.Tensor(-56.44375, shape=(), dtype=float32)\n",
      "tf.Tensor(-56.525963, shape=(), dtype=float32)\n",
      "tf.Tensor(-56.60722, shape=(), dtype=float32)\n",
      "tf.Tensor(-56.681507, shape=(), dtype=float32)\n",
      "tf.Tensor(-56.745857, shape=(), dtype=float32)\n",
      "tf.Tensor(-56.803444, shape=(), dtype=float32)\n",
      "tf.Tensor(-56.85563, shape=(), dtype=float32)\n",
      "tf.Tensor(-56.907448, shape=(), dtype=float32)\n",
      "tf.Tensor(-56.960888, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.014294, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.060265, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.094524, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.12075, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.14319, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.163452, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.18266, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.201916, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.221424, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.23995, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.257435, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.273182, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.287453, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.30047, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.312653, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.32421, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.33495, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.345238, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.355022, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.364063, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.37266, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.380962, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.38891, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.396576, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.403694, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.410652, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.41759, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.424427, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.43114, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.43764, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.44422, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.45094, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.457897, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.465187, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.47284, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.48114, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.49056, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.501617, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.51488, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.53129, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.552288, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.58027, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.618824, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.6737, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-57.754036, shape=(), dtype=float32)\n",
      "tf.Tensor(-57.87382, shape=(), dtype=float32)\n",
      "tf.Tensor(-58.051003, shape=(), dtype=float32)\n",
      "tf.Tensor(-58.31422, shape=(), dtype=float32)\n",
      "tf.Tensor(-58.687675, shape=(), dtype=float32)\n",
      "tf.Tensor(-59.181164, shape=(), dtype=float32)\n",
      "tf.Tensor(-59.758167, shape=(), dtype=float32)\n",
      "tf.Tensor(-60.335144, shape=(), dtype=float32)\n",
      "tf.Tensor(-60.82665, shape=(), dtype=float32)\n",
      "tf.Tensor(-61.169075, shape=(), dtype=float32)\n",
      "tf.Tensor(-61.406532, shape=(), dtype=float32)\n",
      "tf.Tensor(-61.57734, shape=(), dtype=float32)\n",
      "tf.Tensor(-61.748158, shape=(), dtype=float32)\n",
      "tf.Tensor(-61.94344, shape=(), dtype=float32)\n",
      "tf.Tensor(-62.137962, shape=(), dtype=float32)\n",
      "tf.Tensor(-62.31185, shape=(), dtype=float32)\n",
      "tf.Tensor(-62.453747, shape=(), dtype=float32)\n",
      "tf.Tensor(-62.58257, shape=(), dtype=float32)\n",
      "tf.Tensor(-62.71067, shape=(), dtype=float32)\n",
      "tf.Tensor(-62.838734, shape=(), dtype=float32)\n",
      "tf.Tensor(-62.96192, shape=(), dtype=float32)\n",
      "tf.Tensor(-63.059776, shape=(), dtype=float32)\n",
      "tf.Tensor(-63.14272, shape=(), dtype=float32)\n",
      "tf.Tensor(-63.22704, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(200):\n",
    "    ptr.train_batch(X, y, gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = ptr.predict_batch(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=47375435, shape=(200,), dtype=float32, numpy=\n",
       "array([ 1., 22., 44., 87.,  1., 87.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.], dtype=float32)>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
