{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointer Network Training\n",
    "In this notebook we are going to train the pointer network with a 10% sample of the CNN dataset we processed in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pointer_network import PointerNetwork\n",
    "from embedding_helper import load_pretrained_embeddings\n",
    "\n",
    "from pickle import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "We load all the data here. We have X for the processed stories, y for the labels, gen for remembering us if a label is to be generated or to be taken from the input, and y_raw are the same as y, without having indexes pointng to X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('processed/X_train.npy')\n",
    "y_train = np.load('processed/y_train.npy')\n",
    "gen_train = np.load('processed/gen_train.npy')\n",
    "y_raw_train = np.load('processed/y_raw_train.npy')\n",
    "\n",
    "X_val = np.load('processed/X_val.npy')\n",
    "y_val = np.load('processed/y_val.npy')\n",
    "gen_val = np.load('processed/gen_val.npy')\n",
    "y_raw_val = np.load('processed/y_raw_val.npy')\n",
    "\n",
    "X_test = np.load('processed/X_test.npy')\n",
    "y_test = np.load('processed/y_test.npy')\n",
    "gen_test = np.load('processed/gen_test.npy')\n",
    "y_raw_test = np.load('processed/y_raw_test.npy')\n",
    "\n",
    "w2id = load(open('processed/w2id.pkl', 'rb'))\n",
    "id2w = load(open('processed/id2w.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = tf.data.Dataset.from_tensor_slices((X_train, y_train, gen_train))\n",
    "train_generator = train_generator.batch(32)\n",
    "train_generator = train_generator.shuffle(1000)\n",
    "\n",
    "val_generator = tf.data.Dataset.from_tensor_slices((X_val, y_val, gen_val))\n",
    "val_generator = val_generator.batch(32)\n",
    "val_generator = val_generator.shuffle(1000)\n",
    "\n",
    "test_generator = tf.data.Dataset.from_tensor_slices((X_test, y_test, gen_test))\n",
    "test_generator = test_generator.batch(32)\n",
    "test_generator = test_generator.shuffle(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "We instantiate the newtwork with this dataset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_units = 128\n",
    "dec_units = 128\n",
    "voc_size = len(w2id.keys())\n",
    "att_units = 128 \n",
    "switch_units = 128\n",
    "max_len = X_train.shape[1]\n",
    "start_index_token = w2id['<start>']\n",
    "end_index_token = w2id['<end>']\n",
    "padding_char = w2id['<pad>']\n",
    "ptr = PointerNetwork(enc_units, \n",
    "                     dec_units, \n",
    "                     voc_size, \n",
    "                     att_units, \n",
    "                     switch_units, \n",
    "                     max_len, \n",
    "                     start_index_token, \n",
    "                     end_index_token,\n",
    "                     padding_char)\n",
    "\n",
    "ptr.set_embeddings_layer(load_pretrained_embeddings(np.zeros((300,300))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_metrics():\n",
    "    metrics = {'training_loss': [],\n",
    "            'training_acc': [],\n",
    "            'training_top5': []}\n",
    "    return metrics\n",
    "\n",
    "val_names = ['val_loss',\n",
    "              'val_acc',\n",
    "              'val_top_k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_score = 0\n",
    "best_score = 0\n",
    "start = time.clock()\n",
    "end = 0\n",
    "j = 0\n",
    "smooth_window = 25\n",
    "mean = 0\n",
    "\n",
    "val_names = ['val_loss',\n",
    "             'val_acc',\n",
    "             'val_top_k']\n",
    "\n",
    "def init_metrics():\n",
    "    metrics = {'training_loss':[],\n",
    "               'training_acc': [],\n",
    "               'training_top5': []}\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "num_iterations = int((len(train.files) / 2))\n",
    "print('Start training...')\n",
    "print(\"Number of iterations per epoch is: \" + str(num_iterations))\n",
    "print()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Init metrics to log\n",
    "    metrics = init_metrics()\n",
    "    \n",
    "    for iteration, (X, y) in enumerate(train_generator):\n",
    "        prev_time = time.time()\n",
    "        # Do a train step on a single batch\n",
    "        logs = model.train_on_batch(X, y)\n",
    "        c_time = time.time()\n",
    "        \n",
    "        for metric_val, (_, lst) in zip(logs, metrics.items()):\n",
    "            lst.insert(0, metric_val)\n",
    "            \n",
    "        if iteration > smooth_window:\n",
    "            metrics_string = 'Epoch: {}'.format(epoch)\n",
    "            \n",
    "            for m_name, m_lst in  metrics.items():\n",
    "                metrics[m_name] = metrics[m_name][:smooth_window]\n",
    "                tensorboard.on_epoch_end(j, { m_name: np.mean(m_lst) })\n",
    "                metrics_string += '\\t{0}: {1:.2}'.format(m_name, np.mean(m_lst))\n",
    "                \n",
    "            # print progress\n",
    "            mean = progress_eta(iteration + 1 - smooth_window, \n",
    "                                num_iterations-smooth_window,\n",
    "                                prev_time, \n",
    "                                c_time,\n",
    "                                mean, \n",
    "                                metrics_string)\n",
    "            prev_time = c_time\n",
    "            j += 1\n",
    "\n",
    "    total_metrics = {}\n",
    "    mean_metrics = {}\n",
    "    \n",
    "    # Compute validation in batches\n",
    "    for X ,y in validation_generator:\n",
    "        metrics_ = model.evaluate(X, y, verbose=0)\n",
    "        \n",
    "        for i, metric in enumerate(metrics_):\n",
    "            try:\n",
    "                total_metrics[val_names[i]].append(metric)\n",
    "            except:\n",
    "                total_metrics[val_names[i]] = [metric]\n",
    "          \n",
    "    # Average results\n",
    "    for key, total_metric in total_metrics.items():\n",
    "        mean_metrics[key] = np.mean(total_metric)\n",
    "        \n",
    "    # Log on tensorboard\n",
    "    tensorboard.on_epoch_end(epoch, mean_metrics)\n",
    "    \n",
    "    # Check best score and swap if better\n",
    "    current_score = mean_metrics['val_acc']\n",
    "    \n",
    "    print()\n",
    "\n",
    "    if current_score > best_score:\n",
    "        model.save_weights(base_filename + 'weights.' + str(epoch) + '-' + str(current_score) + '.hdf5')\n",
    "        best_score = current_score\n",
    "        print(\"Saved. \")\n",
    "        \n",
    "    print(\"Validation Accuracy in is {0:.6f} at epoch {1}\"\\\n",
    "          .format(np.mean(mean_metrics['val_acc']), epoch))\n",
    "    print(\"Validation Top K Accuracy is {0:.6f} at epoch {1}\"\\\n",
    "          .format(np.mean(mean_metrics['val_top_k']), epoch))\n",
    "\n",
    "\n",
    "tensorboard.on_train_end(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
